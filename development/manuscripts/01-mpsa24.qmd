---
title: "Surprise - They're Different!"
subtitle: "Comparing Frequentist and Bayesian Approaches in Public Policy"
author:
  - name: Stefani Langehennig, PhD 
    orcid: 0000-0002-0897-6556
    email: stefani.langehennig@du.edu
    affiliation: University of Denver
  - name: Zach del Rosario, PhD
    orcid: 0000-0003-4676-1692
    email: zdelrosario@olin.edu
    affiliation: Olin College of Engineering
  - name: Mine Dogucu, PhD
    orcid: 0000-0002-8007-934X
    email: mdogucu@uci.edu
    affiliation: University of California Irvine
abstract: |
  Implementing quantitative methodological techniques is a crucial piece of understanding public policy. While quasi-experimental, spatial (diffusion), and mixed methods are most commonly used when teaching policy studies, little research exists on using Bayesian approaches for policy learning, or how the outcomes from traditional quantitative approaches differ from a Bayesian approach. We propose an applied learning activity for students of public policy that exposes them to Bayesian methods and explores the differences between this statistical paradigm and more commonly used approaches. We do this using a structured interrogation of the [The Climate and Economic Justice Screening Tool](https://screeningtool.geoplatform.gov/en/) (CEJST) and the epistemological framings in the 5E model [@elby2010epistemological]. The activity illustrates the importance of statistical assumptions, and by extension, the impact that different quantitative methods have on understanding public policy. The goal of the study is to introduce students, instructors, and practitioners of public policy to a new way of using statistics, equipping them with the tool set and logical processes necessary to apply either approach as they see fit when studying public policy.
keywords:
  - Active Learning
  - Public Policy
  - Bayesian Statistics
  - Undergraduate Education
number-sections: true
bibliography: bibliography.bib
geometry: margin=1in 
#mainfont: Calibri
fontsize: 12pt
---

## Introduction
Social scientists routinely make use of quantitative methods to understand the complex world around them. Approaches employed range from quasi-experimental, spatial (diffusion), and econometric techniques, to methods that are more qualitative in nature. While Bayesian approaches are not completely missing from public policy and related fields (see @gill2013bayesian; @fienberg2011bayesian), they are underutilized for policy learning among academics and policy specialists. 

In this paper, we propose an applied learning activity for students of public policy - though its applications extend beyond this discipline - that exposes them to Bayesian methods. The activity explores the differences between Bayesian approaches and more commonly used statistical techniques to introduce students, instructors, and practitioners to new and different ways of using statistics to investigate real-world problems. Through the application of this activity, our hope is that using these methods will equip them with the tool set and logical processes necessary to apply different quantitative approaches as they see fit when studying public policy.

What follows is an in-depth examination of how teaching different quantitative methods results in a more robust understanding of public policy. We start by reviewing the research to date on cross-disciplinary approaches to using quantitative methods, as well as how this influences students’ assumptions about their own learning using the epistemological framings in the 5E Model [@elby2010epistemological]. Next, we introduce our applied activity and describe its implementation in the classroom. We then discuss our findings from implementing the activity, followed by our conclusions from the study. 

## Background
Literature review here. High-level framing:

- What’s being/has been done in public policy education
- Cross-disciplinary approaches to quantitative methodology education
  - Focus on social sciences
  - Why it’s important/works
- Epistemological framings/5E Model

Overview of model when ready:

- _Engage_: Get students interested
- _Explore_: Students do self-directed inquiry
- _Explain_: Give students conceptual tools
- _Elaborate_: Let students work with the tools
- _Evaluate_: Assess the learning outcomes



## Applied Activity: Comparing Frequentist & Bayesian Approaches {#sec-applied-activity}
We designed, implemented, and evaluated an activity that highlights the differences between Frequentist and Bayesian statistics. Our primary work included designing an activity that takes students through a structured interrogation of a dataset, with the pilot activity launching during spring of 2024. We evaluated this activity’s impact on students’ statistical knowledge and epistemological framings [@elby2010epistemological], discussed more below.

The activity is focused on a real dataset, discussed more below, to which groups of students are given guided statistical analysis. Students follow a structured process to analyze the dataset and interpret their results. However, different groups receive different versions of the activity: some receive a Frequentist approach, while the others receive a Bayesian approach. By carefully crafting the analyses to reach different conclusions, we aim to surprise students with diverging conclusions. The activity concludes with a final full-group discussion, where we highlight the importance of statistical assumptions, completing the comparison of Frequentist and Bayesian approaches.

The activity learning objectives are three-fold. First, students should be able to evaluate multiple hypotheses using inferential statistics; second, students should be able to connect their evaluation of hypotheses to real-world factors; and third, students should be able to state the primary statistical assumptions for Frequentist and Bayesian inference, and understand how they can lead to different conclusions. These learning objectives stem from our overall learning goal of engineering a “classroom controversy” to motivate students to find their own understanding of how Frequentist and Bayesian assumptions can lead to different conclusions (and by extension, real-world decisionmaking).

### Activty Approach
To implement the activity, there are four primary steps, each discussed at length below:

1. Setting the context for the real world problem the class is exploring
2. Introducing the motivation for the activity (statistical approaches) given the context
3. Doing the applied activity
4. Closing out the activity

The activity is built around the aforementioned 5E Model Approach. For _engage_, the goal is to motivate students with current issues around climate and equity. The _explore_ stage is the opportunity in which students get to do self-directed inquiry. For this activity, that means investigating the real-world dataset provided to them in small groups. _Explain_ gives the students the conceptual tools they need to understand the different statistical approaches. Students will learn the basics of assessing and interpreting a fitted statistical model with the instructor. For _elaborate_, students get to work with the tools, meaning they apply the conceptual tools they learned to the real-world dataset. Finally, _evaluate_ involves the assessment of learning outcomes. Students get the opportunity to reflect on their understanding of the concepts and application they just did through an instructor-facilitated class discussion. 
The 5E Model goes through two loops throughout the activity:

The first loop is focused on the applied learning aspects of the activity, or the application of a statistical approach focused on current issues.
The second loop is focused on the conceptual learning aspects of the activity, or the ownership of the results that students discovered.
The desired goals for each loop in the 5E Model are discussed in more detail below. Note that while the instructor and student-facing files are separated in the run of show, the instructor should also review and use - as much as needed - the student-facing files as they go through the activity.


#### Problem Context
Given our interest in teaching students new approaches to examining real-world public policy problems, we focus our activity on [The Climate and Economic Justice Screening Tool](https://screeningtool.geoplatform.gov/en/) (CEJST) that is the result of President Biden's Executive Order issued in January 2021. The tool is used to identify and subsequently help communities disadvantaged by the burdens stemming from climate change in government social programs. While the data covers a number of burdens (health, transportation, and workforce development, for example), we focus on the sustainability aspects of the tool for our activity, including climate change, energy, and legacy pollution burdens on communities.

We introduce students to this tool before delving into the technical aspects of the activity. 

#### Activity Introduction

#### Activity Application

#### Activity Closing


### Recruitment & Activity Materials
The activity is designed for use in a classroom for non-statistics majors. To pilot the activity among students before disseminating widely, we recruited from two distinct populations at our respective institutions. Students recruited from the University of Denver Daniels College of Business have various majors ranging from Business Information and Analytics to Marketing, while students recruited from Olin College of Engineering have focus on a variety of engineering-related topics. 

For both institutions, participants had to be at least 18 years of age or older and must have completed at least one entry-level statistics course. At the University of Denver, all students enrolled in a business school major must complete three courses that are part of their statistics sequence (INFO 1010, 1020, and 2020). Students who were currently enrolled in the Winter 2024 quarter of INFO 1020 received a Canvas announcement about the opportunity to participate in a research study involving an applied activity about statistical inference outside of normal class hours. XX students volunteered to participate after receiving the Canvas announcement. These students met with Dr. Langehennig and completed the activity over the course of approximately 120 minutes and received pizza at the end of the activity. 

[paragraph here on Olin recruitment]

Materials for the activity are openly available on our [GitHub repository](https://github.com/bayes-bats/tier2-freq-bayes) for instructors to use. Important starter documents include the [run of show](https://github.com/bayes-bats/tier2-freq-bayes/blob/main/development/run-of-show.md), which outlines at a high level the different steps of the activity, as well as the artifacts used in the activity. Further, the learning objectives and details of the activity are fleshed out in the [introduction document](https://github.com/bayes-bats/tier2-freq-bayes/blob/main/development/01-introduction-main.qmd). Finally, instructors can watch a [video overview](https://www.youtube.com/watch?v=dwNLcFqQqnE) of the activity on YouTube as well. 


## Evaluation
In addition to exposing students to different statistical paradigms, we are interested in students' awareness of Bayesian methods and their _epistemological framings_ — their assumptions about the nature and accessibility of "truth" [@elby2010epistemological]. Using pre- and post-pre-activity surveys, we measure students’ self-reported familiarity of fundamental Bayesian ideas. Specifically, we measure their attitudes before the activity, as well as their change in attitudes after the activity, with respect to ideas around statistical inference. The reason for using a post-pre design after the activity is straightforward. We want to capture changes in self-perceived attitudes about a topic by asking participants to consider where they think their beliefs were before the activity, followed by where they think they are now (see @hiebert2014power). The participants give themselves two ratings to capture this before and after reflection, as shown below.

![](01-mpsa-survey-ex1.png){fig-align="center" width=100%}

We used a similar format for each of the learning objectives outlined in @sec-applied-activity. The survey includes the following questions with Likert responses ranging from "Strongly disagree" to "Strongly agree": 

1. _To what degree do you (dis) agree with the following statement: There is no uncertainty in the results of a statistical analysis._
2. _To what degree do you (dis) agree with the following statement: The results of a statistical analysis should not depend on the analyst’s assumptions._
3. _To what degree do you (dis) agree with the following statement: I know how to relate statistical analysis to things in the real world._

Each question that is associated with a learning objectives also gives students an open-ended opportunity to elaborate on their Likert responses. The survey ends by asking students the following open-ended question:

_From the activity, what did you learn about the differences between Frequentist and Bayesian statistics? Please provide as much information as you can._

The post-pre survey is implemented after the activity closing, when students have had the opportunity to talk through the outcomes of each approach with their peers and the instructor. The results of the pilot implementation at the University of Denver and Olin College of Engineering are discussed next. 

### Results
Results from surveys here once activity has been implemented.

Implications of the initial results if we're feeling really ambitious!

## Conclusion

[overview of findings from evaluation (surveys) here]

This activity bridges the gap between the common Frequentist approach oft taught in both undergraduate and graduate statistics classes and the Bayesian paradigm, to which many students have not been exposed. We do this using an applied approach, with an eye towards answering education research questions. While many teaching methods highlight the theoretical similarities and differences between frequentists and Bayesians, our activity moves beyond by grounding the comparison in a real-data application, as well as measuring the impact of applying both frameworks in the classroom. In doing so, we hope to introduce students to a new way of using statistics, equipping them with the tool set and logical processes necessary to apply either the frequentist or Bayesian (or both) approaches as they see fit.

Our activity uses an active learning approach, rather than passive lecture. Active learning has been shown to result in superior learning outcomes for students, particularly those from underrepresented groups [@freeman2014active]. In this way, our proposed activity will promote broader impacts of Bayesian thinking.

Our more speculative research goal—to promote more nuanced epistemological framings among students—has further potential impacts. @elby2010epistemological argue that a "sophisticated" personal epistemology is actually achieved when one has access to multiple epistemological framings and can choose to switch between them based on what is productive for the context at hand. Students who can recognize and critique the assumptions underpinning their analyses (treating them as tentative), but carry out their analyses respecting those analyses (treating them as true) will likely be more effective as practicing statisticians. Getting students to recognize the importance of assumptions—and to practice adopting different assumptions—will be a critical first step in developing these multiple epistemological framings.



