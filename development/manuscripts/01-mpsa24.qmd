---
title: "Surprise - They're Different!"
subtitle: "Comparing Frequentist and Bayesian Approaches in Public Policy"
author:
  - name: Stefani Langehennig, PhD 
    orcid: 0000-0002-0897-6556
    email: stefani.langehennig@du.edu
    affiliation: University of Denver
  - name: Zach del Rosario, PhD
    orcid: 0000-0003-4676-1692
    email: zdelrosario@olin.edu
    affiliation: Olin College of Engineering
  - name: Mine Dogucu, PhD
    orcid: 0000-0002-8007-934X
    email: mdogucu@uci.edu
    affiliation: University of California Irvine
abstract: |
  Implementing quantitative methodological techniques is a crucial piece of understanding public policy. While quasi-experimental, spatial (diffusion), and mixed methods are most commonly used when teaching policy studies, little research exists on using Bayesian approaches for policy learning, or how the outcomes from traditional quantitative approaches differ from a Bayesian approach. We propose an applied learning activity for students of public policy that exposes them to Bayesian methods and explores the differences between this statistical paradigm and more commonly used approaches. We do this using a structured interrogation of the [The Climate and Economic Justice Screening Tool](https://screeningtool.geoplatform.gov/en/) (CEJST) and the epistemological framings in the 5E model [@elby2010epistemological; @duran20045e]. The activity illustrates the importance of statistical assumptions, and by extension, the impact that different quantitative methods have on understanding public policy. The goal of the study is to introduce students, instructors, and practitioners of public policy to a new way of using statistics, equipping them with the tool set and logical processes necessary to apply either approach as they see fit when studying public policy.
keywords:
  - Active Learning
  - Public Policy
  - Bayesian Statistics
  - Undergraduate Education
number-sections: true
bibliography: bibliography.bib
geometry: margin=1in 
#mainfont: Calibri
fontsize: 12pt
---


```{r setup}
#| include: false
#| echo: false
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(broom.mixed)
library(patchwork)

#filename <- "../data/1.0-communities.csv"

mean_Intercept_GLOBAL <- 50
sd_Intercept_GLOBAL <- 12.5
```

## Introduction
Social scientists routinely make use of quantitative methods to understand the complex world around them. Approaches employed range from quasi-experimental, spatial (diffusion), and econometric techniques, to methods that are more qualitative in nature. While Bayesian approaches are not completely missing from public policy and related fields [see @gill2013bayesian; @fienberg2011bayesian], they are underutilized for policy learning among academics and policy specialists. This omission is for a few reasons. First, scholars and practitioners in government and public policy spaces often are not exposed to Bayesian methods when they are taught quantitative techniques [@gill2013bayesian]. Thus, the lack of familiarity means Bayesian techniques are not a regular tool in their methods toolkit. Second, the reliance on infusing subjective priors into the methodological approach tends to be questioned by non-statisticians who have received more traditional Frequentist training in their disciplines [@fienberg2011bayesian; @freedman1997some].

In this paper, we propose an applied learning activity for students of public policy - though its applications extend beyond this discipline - that exposes them to Bayesian methods, focusing on both the theoretical and practical underpinnings of the approach. The activity explores the differences between Bayesian approaches and more commonly used statistical techniques to introduce students, instructors, and practitioners to new and different ways of using statistics to investigate real-world problems. Through the application of this activity, our hope is that using these methods will equip them with the tool set and logical processes necessary to apply different quantitative approaches as they see fit when studying public policy. More importantly, using this activity in social science classrooms will encourage students to interrogate and challenge differences in the assumptions made, and outcomes produced, in the statistical approaches they are taught.

What follows is an in-depth examination of how teaching different quantitative methods results in a more robust understanding of public policy. We start by reviewing the research to date on cross-disciplinary approaches to using quantitative methods, as well as how this influences students’ assumptions about their own learning using the epistemological framings in the 5E Model [@elby2010epistemological; @duran20045e]. Next, we introduce our applied activity and describe its implementation in the classroom. We then discuss our findings from implementing the activity, followed by our conclusions from the study. 

## Statistical Inference for Non-Statistics Disciplines
Conveying complex concepts and analytical techniques is one of the most challenging aspects of teaching [@bates2007teaching]. In addition to distilling topics down to make them accessible to all students, instructors are frequently tasked with ensuring that the topics learned in class translate “out in the wild” when students enter their respective professions. This is true in the social sciences and other non-statistics disciplines, like public policy, where students are tasked with translating what they know into evidence-based practices. While the theories underpinning non-statistics subjects can be challenging to understand, layering the quantitative methods typically needed to test research questions on top of these theories can create added challenges. Too often the methods and theories, alongside the question of usefulness outside of the academy, make it difficult to link analysis and policy practices together [@connelly2021translating]. What is more, students who are not statistics majors frequently become anxious and tend to avoid quantitative methods unless the methods are contextualized and the connection between their subject and technical approach seems clear [@gunn2017embedding]. Nonetheless, quantitative methods - particularly statistical inference - are a must for social science students. Most students take at least a one to three class sequence on quantitative methods as undergraduates. These usually include some form of research methods, followed by an applied class that teaches traditional Frequentist statistical inference (for example, Null Hypothesis Significance Testing (NHST)).

Though they are commonly overlooked relative to other forms of statistical inference, Bayesian approaches do make an appearance in the social sciences. For example, @jackman2000estimation offers Bayesian simulation using Markov chain Monte Carlo (MCMC) algorithms as a unifying quantitative solution to estimating models across several contexts in political science [see also @jackman2000estimationAJPS; @jackman2004bayesian; @jackman2009bayesian]. @shor2007bayesian compare simulations of Bayesian multilevel models to other standard estimators for hierarchical data to showcase the Bayesian model’s flexibility and robustness in time-series cross-sectional data, which are commonly used in political science. Still others use in-person interviews to form elicited priors to better understand trust in judicial systems, bridging the gap between qualitative information and empirical models [@gill2005elicited]. Even in the realm of law, scholars are utilizing techniques such as Bayes factors to buttress forensic evidence in expert testimony in court cases [@kadane2024using].

In public policy, Bayesian techniques have been used to analyze the most critical of events, including the recent Covid-19 pandemic. For example, @wibbens2020covid use Bayesian inference to investigate the effectiveness of Covid-19 control strategies across the world, finding broadly executed “core policies” (staying home, cancelling events, etc.) and compliant jurisdictions somewhat reduced the spread of the virus. @deslatte2018policy use Bayesian multilevel models to show that policymakers in city government use permit delays for land use as a tactic to manage growth. Practitioners of public policy are turning to Bayesian inference to tackle the challenge of interpreting findings from impact evaluations, creating new frameworks, such as BASIE (BAyeSian Interpretation of Estimates) [@deke2022basie], to “improve evidenced-based decision making.”^[ https://www.mathematica.org/features/bayesian-methods]

Given academics and practitioners outside of statistics use Bayesian techniques for their own work, a handful have tried to incorporate Bayesian philosophy and principles into how estimation is taught among their students. For instance, @gill2013bayesian offer an accessible introduction to Bayesian analysis for students of public administration and policy, arguing that understanding the theoretical differences between the two approaches is critical, and that Bayesian techniques are more appropriate for the discipline. @wagner2005bayesian make a similar argument, stressing that Bayesian inference is "better suited" for answering public policy questions, showcasing their application on educational outcomes in public schools. Indeed, they even go so far as to say that the traditional statistical inference methods taught in public administration and policy programs is “defective and should be replaced [@wagner2005bayesian, p. 5]" because they are borrowed from other disciplines and do not necessarily fit their research challenges. Finally, @luque2023bayesian use a series of different scenarios to demonstrate ways in which to incorporate Bayesian methods into real-world social inquiries. They do this by way of inferential comparisons between Frequentists and Bayesians, underscoring  the differences between the two approaches both theoretically and in practice.

One of the primary points stressed in the research advocating for Bayesian inference in non-statistics disciplines is not to simply introduce another applied tool without much thought; rather, offering different philosophies and theoretical approaches in addition to the tool is crucial to employing effective analytical practices. Simply, being able to choose and understand the impact of the methodologies used when approaching a real-world problem is critical. In social science quantitative methodology teachings, intellectual development often starts with a “surface approach”, where students memorize and repeat facts that inform them, rather than engaging with or reflecting on the approaches they have learned [@entwistle1997contrasting; @bates2007teaching]. Our goal, in addition to offering a Bayesian learning activity, is to move beyond the surface approach, empowering students with the knowledge to choose between different statistical approaches based on what is productive for the context at hand. This starts with epistemological framings, or the nature of knowledge and understanding among students [@elby2010epistemological].



We structure the activity that follows around the 5E Model based on constructivist pedagogies [@duran20045e] is as follows:
-	_Engage_: Get students interested
- _Explore_: Students do self-directed inquiry
-	_Explain_: Give students conceptual tools
-	_Elaborate_: Let students work with the tools
-	_Evaluate_: Assess the learning outcomes


Students who can recognize and critique the assumptions underpinning their analyses (treating them as tentative), but carry out their analyses respecting those analyses (treating them as true) will likely be more effective as practicing statisticians. Getting students to recognize the importance of assumptions—and to practice adopting different assumptions—will be a critical first step in developing these multiple epistemological framings.

## Applied Activity: Comparing Frequentist & Bayesian Approaches {#sec-applied-activity}
To alleviate some of the challenges that come with teaching different techniques of statistical inference, scholars have compared Frequentist and Bayesian approaches to one another, highlighting where they are similar and when they diverge [@samaniego2010comparison]. We designed, implemented, and evaluated an activity that does just this, using a public policy example to guide our comparison. Our primary work included designing an activity that takes students through a structured interrogation of a dataset, with the pilot activity launching during spring of 2024. The activity is designed to take place during one or two class sessions. We evaluated this activity’s impact on students’ statistical knowledge and epistemological framings using the 5E Model [@elby2010epistemological], discussed more below.

The activity is focused on a real dataset, discussed more below, to which groups of students are given guided statistical analysis. Students follow a structured process to analyze the dataset and interpret their results. However, different groups receive different versions of the activity: some receive a Frequentist approach, while the others receive a Bayesian approach. By carefully crafting the analyses to reach different conclusions, we aim to surprise students with diverging conclusions. The activity concludes with a final full-group discussion, where we highlight the importance of statistical assumptions, completing the comparison of Frequentist and Bayesian approaches.

The activity learning objectives are three-fold. First, students should be able to evaluate multiple hypotheses using inferential statistics; second, students should be able to connect their evaluation of hypotheses to real-world factors; and third, students should be able to state the primary statistical assumptions for Frequentist and Bayesian inference, and understand how they can lead to different conclusions. These learning objectives stem from our overall learning goal of engineering a “classroom controversy” to motivate students to find their own understanding of how Frequentist and Bayesian assumptions can lead to different conclusions (and by extension, real-world decisionmaking).


__Recruitment__

The activity is designed for use in a classroom for non-statistics majors. To pilot the activity among students before disseminating widely, we recruited from two distinct populations at our respective institutions. Students recruited from the University of Denver Daniels College of Business have various majors ranging from Business Information and Analytics to Marketing, while students recruited from Olin College of Engineering have focus on a variety of engineering-related topics. 

For both institutions, participants had to be at least 18 years of age or older and must have completed at least one entry-level statistics course. At the University of Denver, all students enrolled in a business school major must complete three courses that are part of their statistics sequence (INFO 1010, 1020, and 2020). Students who were currently enrolled in the Winter 2024 quarter of INFO 1020 received a Canvas announcement about the opportunity to participate in a research study involving an applied activity about statistical inference outside of normal class hours. XX students volunteered to participate after receiving the Canvas announcement. These students met with Dr. Langehennig and completed the activity over the course of approximately 120 minutes and received pizza at the end of the activity. 

[PARAGRAPH HERE ON OLIN RECRUITMENT]

__Activity Materials__

All materials were created using the programming language `R` and can be rendered in .html or .pdf format for use. The materials are openly available for instructors on our [GitHub repository](https://github.com/bayes-bats/tier2-freq-bayes). Important starter documents include the [run of show](https://github.com/bayes-bats/tier2-freq-bayes/blob/main/development/run-of-show.md), which outlines at a high level the different steps of the activity, as well as the artifacts used in the activity. Further, the learning objectives and details of the activity are fleshed out in the [introduction document](https://github.com/bayes-bats/tier2-freq-bayes/blob/main/development/01-introduction-main.qmd). Finally, instructors can watch a [video overview](https://www.youtube.com/watch?v=dwNLcFqQqnE) of the activity on YouTube as well.

### Activty Approach {#sec-activity-approach}
To implement the activity, there are four steps, each discussed at length below:

1. Setting the context for the real world problem the class is exploring
2. Introducing the motivation for the activity (statistical approaches) given the context
3. Doing the applied activity
4. Closing out the activity

The activity is built around the aforementioned 5E Model Approach, where two loops of the model are working concurrently. The first loop is focused on the applied learning aspects of the activity, or the application of a statistical approach focused on current issues. For example, for _engage_ the goal is to motivate students with current issues around climate and equity. The _explore_ stage is the opportunity in which students get to do self-directed inquiry. For this activity, that means investigating the real-world dataset provided to them in small groups. _Explain_ gives the students the conceptual tools they need to understand the different statistical approaches. Students will learn the basics of assessing and interpreting a fitted statistical model with the instructor. For _elaborate_, students get to work with the tools, meaning they apply the conceptual tools they learned to the real-world dataset. Finally, _evaluate_ involves giving students the opportunity to reflect on their understanding of the concepts and application they just did through an instructor-facilitated class discussion. 

The second loop is focused on the conceptual learning aspects of the activity, or the ownership of the results that students discovered. _Engage_ is focused on the different questions around the data (context) that should motivate their search for an explanation for outcomes using statistical approaches. _Explore_ and _explain_ capture the introduction to statistical inference broadly defined, the learning objectives for the activity, and the high-level critical differences in Frequentist and Bayesian approaches. For _elaborate_, students articulate the basic concepts of assessing and interpreting a fitted statistical model and come to conclusions related to the research question and hypotheses. Finally, _evaluate_ uses the concepts and what students learned in the application to articulate and refine their understanding of the differences between Frequentist and Bayesian approaches. 

#### Problem Context {#sec-prob-context}
Given our interest in teaching students new approaches to examining real-world public policy problems, we start our activity by introducing [The Climate and Economic Justice Screening Tool](https://screeningtool.geoplatform.gov/en/) (CEJST). The CEJST is the result of President Biden's Executive Order issued in January 2021. The tool is used to identify and subsequently help communities disadvantaged by the burdens stemming from climate change in government social programs. While the data covers a number of burdens (health, transportation, and workforce development, for example), we focus on the sustainability aspects of the tool for our activity, including climate change, energy, and legacy pollution burdens on communities.

We begin with a straightforward explanation of the dataaset, situating it in the contemporary dialogue around climate change. Specifically, we theorize there may be a relationship between climate change burdens and minorities residing in Census tracts. By providing them with this context, we get students to think about possible questions - and by extension, hypotheses - they may be able to explore using statistical inference. 

To dig into the context of our real-world problem further, we also provide embedded code snippets and output from `R` of some high-level exploratory data analysis (EDA) for the students to review and discuss. We begin by focusing our attention on a few variables of interest for EDA. We start with the _energy burden percentile_, which captures the percentile of energy cost as well as energy-related pollution within a census tract, as well as the _percent of African-American or Black alone_, which captures the percent of African-American or Black individuals in a census tract.^[While we guide our students to the variables we want to explore for this implementation of the activity, instructors can modify the activity such that students explore the dataset and identify variables of interest on their own. If they know how to use statistical software, they can also conduct the EDA and analyses in `R` on their own, rather than giving them the completed version as we do here.] The instructor walks the students through basic data wrangling, providing prompt questions to get them thinking about the substantive implications of descriptive statistics and visualizations. Below is an example of the output, which contains the code used to create the figure, as well as a short description and a prompt question for the students.

![](01-mpsa-context-ex1.png){fig-align="center" width=75%}

After students have investigated the dataset and have a more thorough understanding of the problem at hand, the instructor turns their attention to the introduction document, discussed next. 

#### Activity Introduction {#sec-activity-intro}
This portion of the activity is instructor led, with them walking students through the learning objectives and warmup questions, which in turn initiates a group-wide discussion on statistical inference. The instructor also discusses inference at a high level, offering more pointed discussion around crafting a research question and hypotheses. At this point, the instructor turns students to the simplified [critical differences one-pager](https://github.com/bayes-bats/tier2-freq-bayes/blob/main/development/03-simplified-main.qmd) that introduces them to the primary differences between the Frequentist and Bayesian paradigms. To keep the exercise manageable, we focus students' attention on _general inference_ and _model summaries_.^[You can access the [full one-pager](https://github.com/bayes-bats/tier2-freq-bayes/blob/main/development/03-one-pager-main.qmd) that is instructor-facing on the associated GitHub repository. In addition to comparing general inference and model summaries, it also includes comparisons between fixed variables, interpreting probabilities, and model inference.] @tbl-inf and @tbl-summ showcase the differences outlined in the critical differences one-pager between general inference and model summaries:


| Frequentist | Bayesian | 
|-------------|----------|
| Deduction from Pr(data $|$ H0), by setting $\alpha$ in advance | Induction from Pr($\theta$ $|$ data), starting with Pr($\theta$) 
| Accept H1 if Pr(data $|$ H0) < $\alpha$ | 1−$\alpha$% of most likely parameter values fall within a 1−$\alpha$ HPD
| Accept H0 if Pr(data $|$ H0) ≥ $\alpha$|

: General Inference {#tbl-inf}{.sm}


| Frequentist | Bayesian | 
|-------------|----------|
| Point estimates and standard errors | Descriptions of the posterior distribution such as means and quantiles
| 95% confidence intervals indicating that 19/20 times the interval covers the true parameter value | Highest posterior density intervals indicating region of highest posterior probability
|           | 1−$\alpha$% of most likely parameter values fall within a 1−$\alpha$ HPD

: Model Summaries {#tbl-summ}{.sm}

The introductory discussion of the activity wraps up with the instructor introducing the research question and associated hypothesis the class will test with their respective statistical approach. Specifically, students with will be assessing whether Black Americans experience a disproportionate level of energy expenditure using inferential statistics and the CEJST dataset.^[An extension of the activity could have the students develop the research questions and hypotheses on their own. Given the activity we implemented is meant to span one class period, we provide those for the students.]

#### Activity Application {#sec-activity-app}
The activity application step is the heart of the exercise. Students are assigned a random number generated by a random number generator and put into groups based on their number to go through an applied statistical analysis. There are two versions that are circulated: the Frequentist analysis and the Bayesian analysis. The analyses that are given to the students are completed - they only receive the output of the analysis with associated questions to help them think through the different parts of the analysis before they come to any conclusions. 

It is important to note that the data used for each analysis is the exact same for both of the groups, as is the hypothesis that the students are testing. Additionally, the students are asked to assess the same conceptual things, regardless of which activity they receive. They will use the _general inference_ and _model summaries_ comparison discussed in the @sec-activity-intro to diagnose the outputs of the models from the analyses.

__Frequentist Analysis__

Both activities begin with a quick overview of the hypothesis the students are testing, as well as the different components of a statistical model. For the Frequentist model specifically, the instructor introduces the following model, where $B$ as the dependent variable (energy burden percentile), $P$ is the percent black, $m$ is the slope parameter, $b$ is the intercept parameter, and $\epsilon$ captures the error term.

 $${B = m P + b + \epsilon}$$
The instructor encourages the class to think through how to interpret estimates in a linear model using Frequentist statistics, noting a number of important assumptions along the way, including that the $b$ and $m$ parameters are fixed but unknown values. This is a natural place for a number of questions to be asked of the class related to model summaries and general inference. The questions below illustrate what the class is asked for understanding model summaries in a Frequentist model, along with the associated answers:

::: {.callout-note icon="false" title="Questions for the Class"}
- Which scenario gives the largest estimate for the slope?
  - Scenario B
- Does the confidence interval for Scenario A include zero? (NB. A confidence interval includes zero if Lower < 0 < Upper.)
  - Yes
- Does the confidence interval for Scenario B include zero?
  - Yes
- Does the confidence interval for Scenario C include zero?
  - No
- If a confidence interval includes zero, this indicates that we cannot conclude whether the slope is positive or negative. For which scenarios can we not conclude whether the slope is positive or negative?
   - Scenarios A and B 
:::

After students have revisited the concepts around model summaries and general inference for the Frequentist linear model, they move to a predictive model where they are given a number of predicted versus observed plots, show below, for the model across three states: Massachusetts, Colorado, Florida, and the entire sample of data (e.g., the United States). Additionally, they are given the intercept and slope estimates, as well as the confidence intervals for each model.  

![](01-mpsa-freq-ex1.png){fig-align="center" width=50%}

![](01-mpsa-COfreq-ex1.png){fig-align="center" width=50%}

Once they have seen the results for each of the states and the U.S., students are given a set of questions that encourage them to think about the model results know what they do about model summaries and general inference. At this point, the students are exploring the results of the model in their respective groups and discussing and answering together the questions provided to them.

__Bayesian Analysis__

The cadence of the Bayesian analysis largely mirrors the Frequentist analysis to begin. Like the Frequentist groups, the Bayesian groups also revisit the hypothesis they are testing, as well as the different components of a statistical model, though this time it is a Bayesian linear model. 

$$ B = m P + b + \epsilon $$

where $B$ is the energy burden percentile, $P$ is the percent Black, $m$
is the slope parameter, $b$ is the intercept parameter, and $\epsilon$
is a *residual* term that represents factors not accounted in the model.
The residual term is assumed to be normally distributed
$\epsilon \sim N(0, \sigma^2)$ with an unknown parameter $\sigma^2$. All
three parameters have a prior distribution, defined via:

$$
\begin{aligned} m \sim N(\mu_m, \sigma_m^2), \\ b \sim N(\mu_b, \sigma_b^2), \\ \sigma^2 \sim \text{Exponential}(1/s_y), \end{aligned}
$$
where $m, b, \sigma^2$ are independent.[^1] The class discusses how to set
the prior through its parameter values
$\mu_m, \mu_b, \sigma_m^2, \sigma_b^2$ later in the activity. The instructor also discusses model assumptions for the Bayesian approach, highlighting one of the fundamental differences between Frequentists and Bayesians: the intercept $b$ and slope $m$ parameters are treated as random variables with a distribution that represents our state of knowledge.

As with the Frequentist analysis, there are a number of questions asked of the class related to model summaries and general inference for the Bayesians. This time, however, students are shown the posterior distribution for the slope ($m$) and intercept ($b$) for a fitted model using Massachusetts data. The idea here is to get students thinking about how confident they should be in drawing conclusions from the model. After they have seen the distributions for these parameters, they are shown three different posterior distributions for the slope parameter so they can make sense of the results from the posterior:

```{r notional-posteriors}
#| warning: false
#| echo: false
#| fig-align: center
#| fig-height: 3
tibble(x = seq(-25, +75, length.out = 200)) %>% 
  mutate(
    d_A = dnorm(x, mean =  1, sd =  5),
    d_B = dnorm(x, mean = 25, sd = 20),
    d_C = dnorm(x, mean = 25, sd =  5),
  ) %>% 
  pivot_longer(
    cols = contains("d_"),
    names_sep = "_",
    names_to = c(".value", "Posterior")
  ) %>% 
  
  ggplot(aes(x, d)) +
  geom_line() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  facet_grid(~Posterior, labeller = label_both) +
  theme_minimal() +
  labs(
    x = "Slope Parameter",
    y = "Posterior Density",
  )
```

Students are then asked a series of questions, a sample of which are below, to get them thinking about what the posterior distribution captures:

::: {.callout-note icon="false" title="Questions for the Class"}

-   Roughly, what fraction of *Posterior Distribution A* is greater than
    zero?
    -   Precisely 57.93%, so roughly 60%
-   Roughly, what fraction of *Posterior Distribution B* is greater than
    zero?
    -   Precisely 89.44%, so roughly 90%
-   Roughly, what fraction of *Posterior Distribution C* is greater than
    zero?
    -   Essentially 100%
-   Which posterior gives the *highest confidence* (*highest
    probability*) that the slope parameter is positive? How can you
    tell?
    -   Posterior C, as the probability is essentially 100% (as we
        identified above).
:::

[^1]: Note that $s_y$ is determined based on the standard deviation in
    the observed data. This is a way of *autoscaling* the prior.

The students have the opportunity to look more closely at the slope parameter for Massachusetts and discuss it in the context of general inference. The students should recognize that a positive slope is in agreement with the hypothesis they are exploring before moving to the predictive portion of the Bayesian analysis, studying the posterior predictions.

To further stress the differences between the two approaches, the activity notes that a prior distribution must also be provided for all of the components of the model. These priors represent all of our prior knowledge about the real-world problem the students are trying to solve. It also ties all of the pieces together - a fitted Bayesian model is comprised of the _data + prior = posterior_. The activity walks the students through hypothetical scenarios where different prior distributions are use for the slope parameter when fitting the model with the Massachusetts data. This also shows the utility of a Bayesian approach when you have limited data, as it allows us to incorporate prior knowledge, a scenario we use in the activity (e.g., we assume the CEJS data is limited). 

We employ a sequential Bayesian analysis by using the posterior from one analysis as the prior for a new analysis. We provide the following to the students in the Bayesian group and have the students pick a state's prior distribution for the rest of the activity:

::: {.callout-note icon="false" title="Pick a State"}
Study the posteriors above carefully; you will use this as a *prior
distribution* for the slope for the rest of the activity. This means you
will combine *new data* with a *prior distribution* to form a new
*posterior distribution* for the model parameters. The *prior
distribution* should reflect your beliefs about what you think the slope
parameter should be.

Pick *one state for your group*, then come ask the instructor for your
chosen state's packet.
:::

![](01-mpsa-bayesSlope-ex1.png){fig-align="center" width=75%}


This represents a critical step in the activity for the Bayesians - they must pick their prior distribution based on their beliefs about what they think the slope parameter should be. This will subsequently be used  with new data to form a posterior distribution later in the activity. Each of the results are placed in an envelope, only to be opened once selected by a group. The following figure represents an example of the sequential Bayesian analysis used in the full activity that students will have to choose from and interpret. On the left are the results showing the fitted lines and slope posteriors for Florida using priors from Massachusetts, Minnesota, and New Hampshire. On the right are the results showing the fitted lines and slope posteriors for Colorado, also using priors from Massachusetts, Minnesota, and New Hampshire.


![](01-mpsa-bayescomp.png){fig-align="center" width=100%}


#### Activity Closing {#sec-activity-close}
After students have had the opportunity to work through their respective applied analyses and discuss the questions in the associated document, the groups come back together for a full-class discussion. In addition to students jotting down any remaining questions they may have about each of the following questions, they are posed to the whole class for discussion and are tied to the learning objectives in @sec-applied-activity:

- What can we say about our hypothesis?
- How would you answer our research question now that we have analyzed the data?
- What can we conclude about the relationship between sustainability and disadvantaged communities? What might you recommend from a policy-making perspective?

We use these questions for a few reasons. The first is so the entire class can hear the impressions of both groups regarding the statistical approach used in their analyses. The second is to play into the "controversy" or differences between the approaches to further engage students on the importance of assumptions for statistical conclusions. The instructor facilitates a debate between the two groups using the critical differences one-pager discussed in @sec-activity-intro. Each group likely thinks their conclusions are "correct" based on their analyses, however it is important to point out that the controversy cannot be resolved; rather the results from our analyses are conditional on the assumptions chosen for the analysis, implying that choosing appropriate assumptions is critical to a sound analysis.


## Evaluation {#sec-eval}
In addition to exposing students to different statistical paradigms, we are interested in students' awareness of Bayesian methods and their _epistemological framings_ — their assumptions about the nature and accessibility of "truth" [@elby2010epistemological]. Using pre- and post-pre-activity surveys, we measure students’ self-reported familiarity of fundamental Bayesian ideas. Specifically, we measure their attitudes before the activity, as well as their change in attitudes after the activity, with respect to ideas around statistical inference. The reason for using a post-pre design after the activity is straightforward. We want to capture changes in self-perceived attitudes about a topic by asking participants to consider where they think their beliefs were before the activity, followed by where they think they are now (see @hiebert2014power). The participants give themselves two ratings to capture this before and after reflection, as shown below.

![](01-mpsa-survey-ex1.png){fig-align="center" width=75%}

We used a similar format for each of the learning objectives outlined in @sec-applied-activity. The survey includes the following questions with Likert responses ranging from "Strongly disagree" to "Strongly agree": 

1. _To what degree do you (dis) agree with the following statement: There is no uncertainty in the results of a statistical analysis._
2. _To what degree do you (dis) agree with the following statement: The results of a statistical analysis should not depend on the analyst’s assumptions._
<!-- 3. _To what degree do you (dis) agree with the following statement: I know how to relate statistical analysis to things in the real world._ -->

Each question that is associated with a learning objectives also gives students an open-ended opportunity to elaborate on their Likert responses. The survey ends by asking students the following open-ended question:

_From the activity, what did you learn about the differences between Frequentist and Bayesian statistics? Please provide as much information as you can._

The post-pre survey is implemented after the activity closing, when students have had the opportunity to talk through the outcomes of each approach with their peers and the instructor. The results of the pilot implementation at the University of Denver and Olin College of Engineering are discussed next. 

### Results {#sec-results}
Results from surveys here once activity has been implemented.

Implications of the initial results.

## Conclusion {#sec-conclusion}

[overview of findings from evaluation (surveys) here]

This activity bridges the gap between the common Frequentist approach oft taught in both undergraduate and graduate statistics classes and the Bayesian paradigm, to which many students have not been exposed. We do this using an applied approach, with an eye towards answering education research questions. While many teaching methods highlight the theoretical similarities and differences between Frequentists and Bayesians, our activity moves beyond by grounding the comparison in a real-data application, as well as measuring the impact of applying both frameworks in the classroom. In doing so, we hope to introduce students to a new way of using statistics, equipping them with the tool set and logical processes necessary to apply either the Frequentist or Bayesian (or both) approaches as they see fit.

Our activity uses an active learning approach, rather than passive lecture. Active learning has been shown to result in superior learning outcomes for students, particularly those from underrepresented groups [@freeman2014active]. In this way, our proposed activity will promote broader impacts of Bayesian thinking.

Our more speculative research goal—to promote more nuanced epistemological framings among students—has further potential impacts. @elby2010epistemological argue that a "sophisticated" personal epistemology is actually achieved when one has access to multiple epistemological framings and can choose to switch between them based on what is productive for the context at hand. Students who can recognize and critique the assumptions underpinning their analyses (treating them as tentative), but carry out their analyses respecting those analyses (treating them as true) will likely be more effective as practicing statisticians. Getting students to recognize the importance of assumptions—and to practice adopting different assumptions—will be a critical first step in developing these multiple epistemological framings.

## References

::: {#refs}
:::

